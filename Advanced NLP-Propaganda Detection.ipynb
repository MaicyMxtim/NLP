{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "687728e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from transformers import BertForSequenceClassification, BertTokenizer, BertModel, BertForMaskedLM\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "90cd0dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the data\n",
    "train_df = pd.read_csv(\"propaganda_train.tsv\", delimiter='\\t')\n",
    "test_df = pd.read_csv(\"propaganda_val.tsv\", delimiter='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "65a4a565",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>tagged_in_context</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>not_propaganda</td>\n",
       "      <td>No, &lt;BOS&gt; he &lt;EOS&gt; will not be confirmed.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>not_propaganda</td>\n",
       "      <td>This declassification effort &lt;BOS&gt; won’t make ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>flag_waving</td>\n",
       "      <td>The Obama administration misled the &lt;BOS&gt; Amer...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>not_propaganda</td>\n",
       "      <td>“It looks like we’re capturing the demise of t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>not_propaganda</td>\n",
       "      <td>&lt;BOS&gt; Location: Westerville, Ohio &lt;EOS&gt;</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            label                                  tagged_in_context\n",
       "0  not_propaganda         No, <BOS> he <EOS> will not be confirmed. \n",
       "1  not_propaganda  This declassification effort <BOS> won’t make ...\n",
       "2     flag_waving  The Obama administration misled the <BOS> Amer...\n",
       "3  not_propaganda  “It looks like we’re capturing the demise of t...\n",
       "4  not_propaganda           <BOS> Location: Westerville, Ohio <EOS> "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f23bb9f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>tagged_in_context</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>not_propaganda</td>\n",
       "      <td>On average, between 300 and 600 infections are...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>causal_oversimplification</td>\n",
       "      <td>Mostly because &lt;BOS&gt; the country would not las...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>appeal_to_fear_prejudice</td>\n",
       "      <td>Lyndon Johnson &lt;BOS&gt; gets Earl Warren and Sen....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>not_propaganda</td>\n",
       "      <td>&lt;BOS&gt; You &lt;EOS&gt; may opt out at anytime.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>repetition</td>\n",
       "      <td>It must be exacted from him directly in order ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       label  \\\n",
       "0             not_propaganda   \n",
       "1  causal_oversimplification   \n",
       "2   appeal_to_fear_prejudice   \n",
       "3             not_propaganda   \n",
       "4                 repetition   \n",
       "\n",
       "                                   tagged_in_context  \n",
       "0  On average, between 300 and 600 infections are...  \n",
       "1  Mostly because <BOS> the country would not las...  \n",
       "2  Lyndon Johnson <BOS> gets Earl Warren and Sen....  \n",
       "3           <BOS> You <EOS> may opt out at anytime.   \n",
       "4  It must be exacted from him directly in order ...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "250c202f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label                0\n",
       "tagged_in_context    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check for null values\n",
    "train_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f697892f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label                0\n",
       "tagged_in_context    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check for null values\n",
    "test_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "75a691fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: (2414, 2)\n",
      "Test data shape: (580, 2)\n"
     ]
    }
   ],
   "source": [
    "#Size of the data\n",
    "print(\"Training data shape:\", train_df.shape)\n",
    "print(\"Test data shape:\", test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "136ffd78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data label distribution:\n",
      "not_propaganda               1191\n",
      "exaggeration,minimisation     164\n",
      "causal_oversimplification     158\n",
      "name_calling,labeling         157\n",
      "loaded_language               154\n",
      "appeal_to_fear_prejudice      151\n",
      "flag_waving                   148\n",
      "repetition                    147\n",
      "doubt                         144\n",
      "Name: label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#Number of instances for each label\n",
    "print(\"Training data label distribution:\")\n",
    "print(train_df[\"label\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "efbff892",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data label percentages:\n",
      "not_propaganda               49.337200\n",
      "exaggeration,minimisation     6.793703\n",
      "causal_oversimplification     6.545153\n",
      "name_calling,labeling         6.503728\n",
      "loaded_language               6.379453\n",
      "appeal_to_fear_prejudice      6.255178\n",
      "flag_waving                   6.130903\n",
      "repetition                    6.089478\n",
      "doubt                         5.965203\n",
      "Name: label, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "#Percentage of each label\n",
    "print(\"Training data label percentages:\")\n",
    "print(train_df[\"label\"].value_counts(normalize=True)*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdb6661b",
   "metadata": {},
   "source": [
    "## 1: Propaganda vs Not Propaganda\n",
    "### Approach 1: Fine-tuned BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f919e53c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3 - Average training loss: 0.5428\n",
      "Validation Accuracy: 0.8965\n",
      "             Actual       Predicted\n",
      "0    not_propaganda  not_propaganda\n",
      "1    not_propaganda  not_propaganda\n",
      "2        propaganda      propaganda\n",
      "3        propaganda      propaganda\n",
      "4        propaganda      propaganda\n",
      "..              ...             ...\n",
      "478      propaganda      propaganda\n",
      "479      propaganda      propaganda\n",
      "480      propaganda  not_propaganda\n",
      "481  not_propaganda  not_propaganda\n",
      "482      propaganda  not_propaganda\n",
      "\n",
      "[483 rows x 2 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/3 - Average training loss: 0.2917\n",
      "Validation Accuracy: 0.9006\n",
      "             Actual       Predicted\n",
      "0    not_propaganda  not_propaganda\n",
      "1    not_propaganda  not_propaganda\n",
      "2        propaganda      propaganda\n",
      "3        propaganda      propaganda\n",
      "4        propaganda      propaganda\n",
      "..              ...             ...\n",
      "478      propaganda      propaganda\n",
      "479      propaganda      propaganda\n",
      "480      propaganda  not_propaganda\n",
      "481  not_propaganda  not_propaganda\n",
      "482      propaganda  not_propaganda\n",
      "\n",
      "[483 rows x 2 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/3 - Average training loss: 0.1829\n",
      "Validation Accuracy: 0.9337\n",
      "             Actual       Predicted\n",
      "0    not_propaganda  not_propaganda\n",
      "1    not_propaganda  not_propaganda\n",
      "2        propaganda      propaganda\n",
      "3        propaganda      propaganda\n",
      "4        propaganda      propaganda\n",
      "..              ...             ...\n",
      "478      propaganda      propaganda\n",
      "479      propaganda      propaganda\n",
      "480      propaganda  not_propaganda\n",
      "481  not_propaganda      propaganda\n",
      "482      propaganda  not_propaganda\n",
      "\n",
      "[483 rows x 2 columns]\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "not_propaganda       0.94      0.93      0.93       241\n",
      "    propaganda       0.93      0.94      0.93       242\n",
      "\n",
      "      accuracy                           0.93       483\n",
      "     macro avg       0.93      0.93      0.93       483\n",
      "  weighted avg       0.93      0.93      0.93       483\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW, get_linear_schedule_with_warmup\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# Load the data\n",
    "train_df = pd.read_csv(\"propaganda_train.tsv\", delimiter='\\t')\n",
    "test_df = pd.read_csv(\"propaganda_val.tsv\", delimiter='\\t')\n",
    "\n",
    "# Split the training data into training and validation sets\n",
    "train_df, val_df = train_test_split(train_df, test_size=0.2, random_state=42)\n",
    "\n",
    "# Preprocess the data\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "def preprocess_data(data, max_length=128):\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "    labels = []\n",
    "\n",
    "    for _, row in data.iterrows():\n",
    "        encoded_dict = tokenizer.encode_plus(\n",
    "            row['tagged_in_context'],\n",
    "            add_special_tokens=True,\n",
    "            max_length=64,\n",
    "            padding=\"max_length\",\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "            truncation=True\n",
    "        )\n",
    "        input_ids.append(encoded_dict['input_ids'])\n",
    "        attention_masks.append(encoded_dict['attention_mask'])\n",
    "        labels.append(1 if row['label'] != 'not_propaganda' else 0)\n",
    "\n",
    "    input_ids = torch.cat(input_ids, dim=0)\n",
    "    attention_masks = torch.cat(attention_masks, dim=0)\n",
    "    labels = torch.tensor(labels)\n",
    "\n",
    "    return input_ids, attention_masks, labels\n",
    "\n",
    "train_input_ids, train_attention_masks, train_labels = preprocess_data(train_df)\n",
    "val_input_ids, val_attention_masks, val_labels = preprocess_data(val_df)\n",
    "test_input_ids, test_attention_masks, test_labels = preprocess_data(test_df)\n",
    "\n",
    "# Load the pre-trained BERT model\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    \"bert-base-uncased\",\n",
    "    num_labels=2,\n",
    "    output_attentions=False,\n",
    "    output_hidden_states=False,\n",
    ")\n",
    "\n",
    "# Fine-tune the model\n",
    "batch_size = 32\n",
    "epochs = 3\n",
    "\n",
    "train_data = TensorDataset(train_input_ids, train_attention_masks, train_labels)\n",
    "train_sampler = torch.utils.data.RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "val_data = TensorDataset(val_input_ids, val_attention_masks, val_labels)\n",
    "val_sampler = torch.utils.data.SequentialSampler(val_data)\n",
    "val_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=batch_size)\n",
    "\n",
    "test_data = TensorDataset(test_input_ids, test_attention_masks, test_labels)\n",
    "test_sampler = torch.utils.data.SequentialSampler(test_data)\n",
    "test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5, eps=1e-8)\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    progress_bar = tqdm(train_dataloader, desc=f'Epoch {epoch + 1}/{epochs}', leave=False)\n",
    "\n",
    "    for batch in progress_bar:\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        input_ids, attention_masks, labels = batch\n",
    "\n",
    "        model.zero_grad()\n",
    "        outputs = model(input_ids, attention_mask=attention_masks, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        progress_bar.set_postfix({'training_loss': loss.item()})\n",
    "\n",
    "    avg_train_loss = total_loss / len(train_dataloader)\n",
    "    print(f\"Epoch {epoch + 1}/{epochs} - Average training loss: {avg_train_loss:.4f}\")\n",
    "\n",
    "    model.eval()\n",
    "    val_preds = []\n",
    "    val_true_labels = []\n",
    "\n",
    "    for batch in val_dataloader:\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        input_ids, attention_masks, labels = batch\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids, attention_mask=attention_masks)\n",
    "\n",
    "        logits = outputs.logits\n",
    "        batch_preds = torch.argmax(logits, dim=1).detach().cpu().numpy()\n",
    "        batch_true_labels = labels.detach().cpu().numpy()\n",
    "\n",
    "        val_preds.extend(batch_preds)\n",
    "        val_true_labels.extend(batch_true_labels)\n",
    "\n",
    "    # Calculate validation accuracy before mapping predicted values to labels\n",
    "    val_accuracy = np.mean(np.array(val_preds) == np.array(val_true_labels))\n",
    "    print(f\"Validation Accuracy: {val_accuracy:.4f}\")\n",
    "    \n",
    "    # Map actual labels to labels\n",
    "    actual_labels_mapped = ['not_propaganda' if label == 0 else 'propaganda' for label in val_true_labels]\n",
    "\n",
    "    # Map predicted values to labels\n",
    "    predicted_labels = ['not_propaganda' if pred == 0 else 'propaganda' for pred in val_preds]\n",
    "\n",
    "    # Create pandas Series for predicted and actual labels\n",
    "    actual_labels_series = pd.Series(actual_labels_mapped, name='Actual')\n",
    "    val_preds_series = pd.Series(predicted_labels, name='Predicted')\n",
    "\n",
    "    # Concatenate the actual and predicted labels into a DataFrame\n",
    "    results_df = pd.concat([actual_labels_series, val_preds_series], axis=1)\n",
    "\n",
    "    print(results_df)\n",
    "\n",
    "# Classification report\n",
    "print(classification_report(actual_labels_series, val_preds_series))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1bfec04",
   "metadata": {},
   "source": [
    "#### Model Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "780be93a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.9103\n",
      "             Actual       Predicted\n",
      "0    not_propaganda  not_propaganda\n",
      "1        propaganda      propaganda\n",
      "2        propaganda      propaganda\n",
      "3    not_propaganda  not_propaganda\n",
      "4        propaganda      propaganda\n",
      "..              ...             ...\n",
      "575  not_propaganda  not_propaganda\n",
      "576  not_propaganda  not_propaganda\n",
      "577  not_propaganda  not_propaganda\n",
      "578      propaganda      propaganda\n",
      "579      propaganda      propaganda\n",
      "\n",
      "[580 rows x 2 columns]\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "not_propaganda       0.94      0.89      0.91       301\n",
      "    propaganda       0.88      0.94      0.91       279\n",
      "\n",
      "      accuracy                           0.91       580\n",
      "     macro avg       0.91      0.91      0.91       580\n",
      "  weighted avg       0.91      0.91      0.91       580\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define a function for testing\n",
    "def test_model(model, test_dataloader):\n",
    "    model.eval()\n",
    "    test_preds = []\n",
    "    test_true_labels = []\n",
    "\n",
    "    for batch in test_dataloader:\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        input_ids, attention_masks, labels = batch\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids, attention_mask=attention_masks)\n",
    "\n",
    "        logits = outputs.logits\n",
    "        batch_preds = torch.argmax(logits, dim=1).detach().cpu().numpy()\n",
    "        batch_true_labels = labels.detach().cpu().numpy()\n",
    "\n",
    "        test_preds.extend(batch_preds)\n",
    "        test_true_labels.extend(batch_true_labels)\n",
    "\n",
    "    # Calculate test accuracy before mapping predicted values to labels\n",
    "    test_accuracy = np.mean(np.array(test_preds) == np.array(test_true_labels))\n",
    "    print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "    # Map actual labels to labels\n",
    "    actual_labels_mapped = ['not_propaganda' if label == 0 else 'propaganda' for label in test_true_labels]\n",
    "\n",
    "    # Map predicted values to labels\n",
    "    predicted_labels = ['not_propaganda' if pred == 0 else 'propaganda' for pred in test_preds]\n",
    "\n",
    "    # Create pandas Series for predicted and actual labels\n",
    "    actual_labels_series = pd.Series(actual_labels_mapped, name='Actual')\n",
    "    test_preds_series = pd.Series(predicted_labels, name='Predicted')\n",
    "\n",
    "    # Concatenate the actual and predicted labels into a DataFrame\n",
    "    results_df = pd.concat([actual_labels_series, test_preds_series], axis=1)\n",
    "\n",
    "    print(results_df)\n",
    "\n",
    "    # Classification report for test dataset\n",
    "    print(classification_report(actual_labels_series, test_preds_series))\n",
    "\n",
    "# Call the function to test the model\n",
    "test_model(model, test_dataloader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1a62fac",
   "metadata": {},
   "source": [
    "### Approach 2: Convolutional Neural Network (CNN) with GloVe Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c2199c95",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-13 11:42:20.260512: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense, Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0f647b82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/maicymaritim/anaconda3/lib/python3.11/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - accuracy: 0.6019 - loss: 0.7063 - val_accuracy: 0.7930 - val_loss: 0.4443\n",
      "Epoch 2/5\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.8376 - loss: 0.3913 - val_accuracy: 0.8427 - val_loss: 0.3554\n",
      "Epoch 3/5\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.8933 - loss: 0.2760 - val_accuracy: 0.8571 - val_loss: 0.3455\n",
      "Epoch 4/5\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9310 - loss: 0.1773 - val_accuracy: 0.8592 - val_loss: 0.3487\n",
      "Epoch 5/5\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9725 - loss: 0.1053 - val_accuracy: 0.8116 - val_loss: 0.4758\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7677 - loss: 0.5930\n",
      "Validation Loss: 0.4758431017398834\n",
      "Validation Accuracy: 0.8115941882133484\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
      "           Actual       Predicted\n",
      "0  not_propaganda  not_propaganda\n",
      "1  not_propaganda      propaganda\n",
      "2      propaganda      propaganda\n",
      "3      propaganda      propaganda\n",
      "4      propaganda  not_propaganda\n",
      "5  not_propaganda  not_propaganda\n",
      "6      propaganda  not_propaganda\n",
      "7      propaganda      propaganda\n",
      "8  not_propaganda  not_propaganda\n",
      "9      propaganda      propaganda\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.93      0.83       241\n",
      "           1       0.91      0.69      0.79       242\n",
      "\n",
      "    accuracy                           0.81       483\n",
      "   macro avg       0.83      0.81      0.81       483\n",
      "weighted avg       0.83      0.81      0.81       483\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Preprocess the text data\n",
    "def preprocess_text(text):\n",
    "    return text.lower()\n",
    "\n",
    "train_df[\"text\"] = train_df[\"tagged_in_context\"].apply(preprocess_text)\n",
    "test_df[\"text\"] = test_df[\"tagged_in_context\"].apply(preprocess_text)\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(train_df[\"text\"], \n",
    "                                                                    train_df[\"label\"], \n",
    "                                                                    test_size=0.2, \n",
    "                                                                    random_state=42)\n",
    "\n",
    "# Tokenize the text data\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(train_texts)\n",
    "\n",
    "train_sequences = tokenizer.texts_to_sequences(train_texts)\n",
    "val_sequences = tokenizer.texts_to_sequences(val_texts)\n",
    "\n",
    "# Pad the sequences to a fixed length\n",
    "max_length = 100\n",
    "train_data = pad_sequences(train_sequences, maxlen=max_length)\n",
    "val_data = pad_sequences(val_sequences, maxlen=max_length)\n",
    "\n",
    "# Prepare the labels\n",
    "train_labels = np.where(train_labels == \"not_propaganda\", 0, 1)\n",
    "val_labels = np.where(val_labels == \"not_propaganda\", 0, 1)\n",
    "\n",
    "# Load the GloVe word embeddings\n",
    "embedding_dim = 100\n",
    "glove_file = \"glove.6B.100d.txt\"  # Download the GloVe embeddings file\n",
    "\n",
    "embeddings_index = {}\n",
    "with open(glove_file, encoding=\"utf8\") as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype=\"float32\")\n",
    "        embeddings_index[word] = coefs\n",
    "\n",
    "# Create the embedding matrix\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "\n",
    "# Build the CNN model\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, embedding_dim, weights=[embedding_matrix], input_length=max_length, trainable=False))\n",
    "model.add(Conv1D(128, 5, activation=\"relu\"))\n",
    "model.add(GlobalMaxPooling1D())\n",
    "model.add(Dense(64, activation=\"relu\"))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "# Train the model\n",
    "epochs = 5\n",
    "batch_size = 32\n",
    "history = model.fit(train_data, train_labels, validation_data=(val_data, val_labels), epochs=epochs, batch_size=batch_size)\n",
    "\n",
    "# Evaluate the model on the validation set\n",
    "loss, accuracy = model.evaluate(val_data, val_labels, batch_size=batch_size)\n",
    "print(\"Validation Loss:\", loss)\n",
    "print(\"Validation Accuracy:\", accuracy)\n",
    "\n",
    "# Predict labels for validation data\n",
    "predictions = model.predict(val_data)\n",
    "predicted_labels = np.round(predictions).astype(int).flatten()\n",
    "\n",
    "# Map actual labels to labels\n",
    "actual_labels_mapped = ['not_propaganda' if label == 0 else 'propaganda' for label in val_labels]\n",
    "\n",
    "# Map predicted values to labels\n",
    "predicted_labels_mapped = ['not_propaganda' if pred == 0 else 'propaganda' for pred in predicted_labels]\n",
    "\n",
    "# Convert predicted labels and validation labels to pandas Series\n",
    "val_labels_series = pd.Series(actual_labels_mapped, name='Actual')\n",
    "predicted_labels_series = pd.Series(predicted_labels_mapped, name='Predicted')\n",
    "\n",
    "# Concatenate the actual and predicted labels into a DataFrame\n",
    "results_df = pd.concat([val_labels_series, predicted_labels_series], axis=1)\n",
    "\n",
    "print(results_df.head(10))\n",
    "\n",
    "# Classification report\n",
    "print(classification_report(val_labels, predicted_labels))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d75a3387",
   "metadata": {},
   "source": [
    "#### Model Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9cd9ed04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "           Actual       Predicted\n",
      "0  not_propaganda  not_propaganda\n",
      "1      propaganda      propaganda\n",
      "2      propaganda  not_propaganda\n",
      "3  not_propaganda  not_propaganda\n",
      "4      propaganda      propaganda\n",
      "5      propaganda      propaganda\n",
      "6      propaganda  not_propaganda\n",
      "7  not_propaganda  not_propaganda\n",
      "8      propaganda  not_propaganda\n",
      "9      propaganda      propaganda\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.95      0.83       301\n",
      "           1       0.92      0.64      0.75       279\n",
      "\n",
      "    accuracy                           0.80       580\n",
      "   macro avg       0.83      0.79      0.79       580\n",
      "weighted avg       0.82      0.80      0.79       580\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Preprocess the text data\n",
    "test_df[\"text\"] = test_df[\"tagged_in_context\"].apply(preprocess_text)\n",
    "\n",
    "# Tokenize the text data for test set\n",
    "test_sequences = tokenizer.texts_to_sequences(test_df[\"text\"])\n",
    "test_data = pad_sequences(test_sequences, maxlen=max_length)\n",
    "\n",
    "# Prepare the labels for test set\n",
    "test_labels = np.where(test_df[\"label\"] == \"not_propaganda\", 0, 1)\n",
    "\n",
    "# Predict labels for test data\n",
    "predictions = model.predict(test_data)\n",
    "predicted_labels = np.round(predictions).astype(int).flatten()\n",
    "\n",
    "# Map actual labels to labels for test set\n",
    "actual_labels_mapped = ['not_propaganda' if label == 0 else 'propaganda' for label in test_labels]\n",
    "\n",
    "# Map predicted values to labels for test set\n",
    "predicted_labels_mapped = ['not_propaganda' if pred == 0 else 'propaganda' for pred in predicted_labels]\n",
    "\n",
    "# Convert predicted labels and test labels to pandas Series\n",
    "test_labels_series = pd.Series(actual_labels_mapped, name='Actual')\n",
    "predicted_labels_series = pd.Series(predicted_labels_mapped, name='Predicted')\n",
    "\n",
    "# Concatenate the actual and predicted labels into a DataFrame for test set\n",
    "test_results_df = pd.concat([test_labels_series, predicted_labels_series], axis=1)\n",
    "\n",
    "print(test_results_df.head(10))\n",
    "\n",
    "# Classification report for test dataset\n",
    "print(classification_report(test_labels, predicted_labels))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c75ba916",
   "metadata": {},
   "source": [
    "## Task 2: Propaganda Technique Classification\n",
    "### Approach 1: Fine-tuned BERT "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "448db0f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 - Train Loss: 2.1671\n",
      "Validation Accuracy: 0.4928\n",
      "                           precision    recall  f1-score   support\n",
      "\n",
      " appeal_to_fear_prejudice       0.26      0.38      0.31        29\n",
      "causal_oversimplification       0.21      0.83      0.33        23\n",
      "                    doubt       0.64      0.21      0.31        34\n",
      "exaggeration,minimisation       0.15      0.09      0.11        35\n",
      "              flag_waving       0.43      0.38      0.40        32\n",
      "          loaded_language       0.00      0.00      0.00        39\n",
      "    name_calling,labeling       0.11      0.26      0.16        23\n",
      "           not_propaganda       0.81      0.74      0.77       241\n",
      "               repetition       0.20      0.04      0.06        27\n",
      "\n",
      "                 accuracy                           0.49       483\n",
      "                macro avg       0.31      0.32      0.27       483\n",
      "             weighted avg       0.53      0.49      0.49       483\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/5 - Train Loss: 1.9083\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/maicymaritim/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/maicymaritim/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/maicymaritim/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.5135\n",
      "                           precision    recall  f1-score   support\n",
      "\n",
      " appeal_to_fear_prejudice       0.34      0.34      0.34        29\n",
      "causal_oversimplification       0.40      0.26      0.32        23\n",
      "                    doubt       0.40      0.29      0.34        34\n",
      "exaggeration,minimisation       0.00      0.00      0.00        35\n",
      "              flag_waving       0.32      0.53      0.40        32\n",
      "          loaded_language       0.12      0.13      0.12        39\n",
      "    name_calling,labeling       0.15      0.65      0.25        23\n",
      "           not_propaganda       0.87      0.77      0.81       241\n",
      "               repetition       0.00      0.00      0.00        27\n",
      "\n",
      "                 accuracy                           0.51       483\n",
      "                macro avg       0.29      0.33      0.29       483\n",
      "             weighted avg       0.54      0.51      0.51       483\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/5 - Train Loss: 1.5324\n",
      "Validation Accuracy: 0.5880\n",
      "                           precision    recall  f1-score   support\n",
      "\n",
      " appeal_to_fear_prejudice       0.46      0.55      0.50        29\n",
      "causal_oversimplification       0.37      0.74      0.49        23\n",
      "                    doubt       0.67      0.12      0.20        34\n",
      "exaggeration,minimisation       0.40      0.06      0.10        35\n",
      "              flag_waving       0.52      0.50      0.51        32\n",
      "          loaded_language       0.29      0.26      0.27        39\n",
      "    name_calling,labeling       0.17      0.43      0.25        23\n",
      "           not_propaganda       0.89      0.83      0.86       241\n",
      "               repetition       0.22      0.37      0.28        27\n",
      "\n",
      "                 accuracy                           0.59       483\n",
      "                macro avg       0.44      0.43      0.38       483\n",
      "             weighted avg       0.64      0.59      0.59       483\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/5 - Train Loss: 1.1777\n",
      "Validation Accuracy: 0.5921\n",
      "                           precision    recall  f1-score   support\n",
      "\n",
      " appeal_to_fear_prejudice       0.34      0.55      0.42        29\n",
      "causal_oversimplification       0.31      0.96      0.46        23\n",
      "                    doubt       0.50      0.09      0.15        34\n",
      "exaggeration,minimisation       0.67      0.17      0.27        35\n",
      "              flag_waving       0.55      0.50      0.52        32\n",
      "          loaded_language       0.38      0.26      0.31        39\n",
      "    name_calling,labeling       0.33      0.43      0.38        23\n",
      "           not_propaganda       0.93      0.80      0.86       241\n",
      "               repetition       0.18      0.37      0.24        27\n",
      "\n",
      "                 accuracy                           0.59       483\n",
      "                macro avg       0.47      0.46      0.40       483\n",
      "             weighted avg       0.68      0.59      0.60       483\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/5 - Train Loss: 0.8490\n",
      "Validation Accuracy: 0.6522\n",
      "                           precision    recall  f1-score   support\n",
      "\n",
      " appeal_to_fear_prejudice       0.38      0.62      0.47        29\n",
      "causal_oversimplification       0.40      0.61      0.48        23\n",
      "                    doubt       0.48      0.38      0.43        34\n",
      "exaggeration,minimisation       0.44      0.49      0.46        35\n",
      "              flag_waving       0.50      0.69      0.58        32\n",
      "          loaded_language       0.44      0.18      0.25        39\n",
      "    name_calling,labeling       0.27      0.52      0.35        23\n",
      "           not_propaganda       0.97      0.86      0.91       241\n",
      "               repetition       0.33      0.19      0.24        27\n",
      "\n",
      "                 accuracy                           0.65       483\n",
      "                macro avg       0.47      0.50      0.46       483\n",
      "             weighted avg       0.69      0.65      0.66       483\n",
      "\n",
      "Test Accuracy: 0.6517\n",
      "                           precision    recall  f1-score   support\n",
      "\n",
      " appeal_to_fear_prejudice       0.47      0.67      0.55        43\n",
      "causal_oversimplification       0.45      0.45      0.45        31\n",
      "                    doubt       0.49      0.61      0.54        38\n",
      "exaggeration,minimisation       0.21      0.32      0.25        28\n",
      "              flag_waving       0.47      0.69      0.56        39\n",
      "          loaded_language       0.25      0.14      0.18        37\n",
      "    name_calling,labeling       0.44      0.45      0.44        31\n",
      "           not_propaganda       0.95      0.83      0.89       301\n",
      "               repetition       0.27      0.19      0.22        32\n",
      "\n",
      "                 accuracy                           0.65       580\n",
      "                macro avg       0.44      0.48      0.45       580\n",
      "             weighted avg       0.68      0.65      0.66       580\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertModel, AdamW\n",
    "from collections import Counter\n",
    "import warnings\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "\n",
    "# Calculate class weights\n",
    "class_distribution = Counter(train_df[\"label\"])\n",
    "total_samples = sum(class_distribution.values())\n",
    "class_weights = {cls: total_samples / (len(class_distribution) * count) for cls, count in class_distribution.items()}\n",
    "\n",
    "idx_to_label = {0: 'not_propaganda', 1: 'exaggeration,minimisation', 2: 'causal_oversimplification',\n",
    "                3: 'name_calling,labeling', 4: 'loaded_language', 5: 'appeal_to_fear_prejudice',\n",
    "                6: 'flag_waving', 7: 'repetition', 8: 'doubt', 9: 'not_propaganda'}\n",
    "\n",
    "\n",
    "# Define label-to-id and id-to-label mappings\n",
    "label_to_id = {label: idx for idx, label in enumerate(class_distribution.keys())}\n",
    "id_to_label = {idx: label for label, idx in label_to_id.items()}\n",
    "\n",
    "# Preprocess the data\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Define the tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Preprocess the data\n",
    "def tokenize_data(data, tokenizer, max_length):\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "    labels = []\n",
    "\n",
    "    for _, row in data.iterrows():\n",
    "        encoded_dict = tokenizer.encode_plus(\n",
    "            row['tagged_in_context'],\n",
    "            add_special_tokens=True,\n",
    "            max_length=max_length,\n",
    "            padding='max_length',\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "            truncation=True\n",
    "        )\n",
    "        input_ids.append(encoded_dict['input_ids'])\n",
    "        attention_masks.append(encoded_dict['attention_mask'])\n",
    "        labels.append(label_to_id[row['label']])\n",
    "\n",
    "    input_ids = torch.cat(input_ids, dim=0)\n",
    "    attention_masks = torch.cat(attention_masks, dim=0)\n",
    "    labels = torch.tensor(labels)\n",
    "\n",
    "    return input_ids, attention_masks, labels\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "train_data, val_data = train_test_split(train_df, test_size=0.2, random_state=42)\n",
    "\n",
    "# Tokenize the text data for train and validation sets\n",
    "max_length = 128  # Adjust as needed\n",
    "train_input_ids, train_attention_masks, train_labels = tokenize_data(train_data, tokenizer, max_length)\n",
    "val_input_ids, val_attention_masks, val_labels = tokenize_data(val_data, tokenizer, max_length)\n",
    "\n",
    "# Define DataLoader for training and validation sets\n",
    "batch_size = 32  # \n",
    "train_dataset = TensorDataset(train_input_ids, train_attention_masks, train_labels)\n",
    "val_dataset = TensorDataset(val_input_ids, val_attention_masks, val_labels)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "\n",
    "# Define the model architecture\n",
    "class BertForTC(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(BertForTC, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "        self.fc = nn.Linear(self.bert.config.hidden_size, 128)\n",
    "        self.classifier = nn.Linear(128, num_classes)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs[1]\n",
    "        pooled_output = self.fc(pooled_output)\n",
    "        logits = self.classifier(pooled_output)\n",
    "        return logits\n",
    "\n",
    "# Initialize the model\n",
    "num_classes = len(label_to_id)\n",
    "model = BertForTC(num_classes)\n",
    "\n",
    "# Set up the optimizer and loss function with class weights\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "class_weights_tensor = torch.tensor([class_weights[idx_to_label[i]] for i in range(num_classes)]).float().to(device)\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights_tensor)\n",
    "\n",
    "# Train the model\n",
    "num_epochs = 5\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    \n",
    "    progress_bar = tqdm(train_dataloader, desc=f'Epoch {epoch + 1}/{num_epochs}', leave=False)\n",
    "\n",
    "    for batch in progress_bar:\n",
    "        input_ids, attention_masks, labels = batch\n",
    "        input_ids = input_ids.to(device)\n",
    "        attention_masks = attention_masks.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids, attention_masks)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        progress_bar.set_postfix({'train_loss': train_loss / len(train_dataloader)})\n",
    "        \n",
    "    average_train_loss = train_loss / len(train_dataloader)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} - Train Loss: {average_train_loss:.4f}\")\n",
    "\n",
    "    # Evaluate the model on the validation set\n",
    "    model.eval()\n",
    "    val_preds = []\n",
    "    val_true_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in val_dataloader:\n",
    "            input_ids, attention_masks, labels = batch\n",
    "            input_ids = input_ids.to(device)\n",
    "            attention_masks = attention_masks.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(input_ids, attention_masks)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "            val_preds.extend(predicted.cpu().numpy())\n",
    "            val_true_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    val_accuracy = accuracy_score(val_true_labels, val_preds)\n",
    "    print(f\"Validation Accuracy: {val_accuracy:.4f}\")\n",
    "\n",
    "    # Convert predicted labels from integers to technique names\n",
    "    predicted_labels_mapped = [id_to_label[label] for label in val_preds]\n",
    "    true_labels_mapped = [id_to_label[label] for label in val_true_labels]\n",
    "\n",
    "    # Print classification report\n",
    "    print(classification_report(true_labels_mapped, predicted_labels_mapped))\n",
    "\n",
    "# Test the model on the test set\n",
    "test_input_ids, test_attention_masks, test_labels = tokenize_data(test_df, tokenizer, max_length)\n",
    "test_dataset = TensorDataset(test_input_ids, test_attention_masks, test_labels)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "model.eval()\n",
    "test_preds = []\n",
    "test_true_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_dataloader:\n",
    "        input_ids, attention_masks, labels = batch\n",
    "        input_ids = input_ids.to(device)\n",
    "        attention_masks = attention_masks.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        outputs = model(input_ids, attention_masks)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "        test_preds.extend(predicted.cpu().numpy())\n",
    "        test_true_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "test_accuracy = accuracy_score(test_true_labels, test_preds)\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "# Convert predicted labels from integers to technique names\n",
    "predicted_labels_mapped = [id_to_label[label] for label in test_preds]\n",
    "true_labels_mapped = [id_to_label[label] for label in test_true_labels]\n",
    "\n",
    "# Print classification report for test set\n",
    "print(classification_report(true_labels_mapped, predicted_labels_mapped))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31860f17",
   "metadata": {},
   "source": [
    "#### Model Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cac61b81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.6517\n",
      "                           precision    recall  f1-score   support\n",
      "\n",
      " appeal_to_fear_prejudice       0.47      0.67      0.55        43\n",
      "causal_oversimplification       0.45      0.45      0.45        31\n",
      "                    doubt       0.49      0.61      0.54        38\n",
      "exaggeration,minimisation       0.21      0.32      0.25        28\n",
      "              flag_waving       0.47      0.69      0.56        39\n",
      "          loaded_language       0.25      0.14      0.18        37\n",
      "    name_calling,labeling       0.44      0.45      0.44        31\n",
      "           not_propaganda       0.95      0.83      0.89       301\n",
      "               repetition       0.27      0.19      0.22        32\n",
      "\n",
      "                 accuracy                           0.65       580\n",
      "                macro avg       0.44      0.48      0.45       580\n",
      "             weighted avg       0.68      0.65      0.66       580\n",
      "\n",
      "                      Actual                  Predicted\n",
      "0             not_propaganda             not_propaganda\n",
      "1  causal_oversimplification                flag_waving\n",
      "2   appeal_to_fear_prejudice   appeal_to_fear_prejudice\n",
      "3             not_propaganda             not_propaganda\n",
      "4                 repetition      name_calling,labeling\n",
      "5      name_calling,labeling                 repetition\n",
      "6            loaded_language                 repetition\n",
      "7             not_propaganda             not_propaganda\n",
      "8                flag_waving                flag_waving\n",
      "9                      doubt  causal_oversimplification\n"
     ]
    }
   ],
   "source": [
    "# Tokenize the text data\n",
    "max_length = 128  #\n",
    "test_input_ids, test_attention_masks, test_labels = tokenize_data(test_df, tokenizer, max_length)\n",
    "test_dataset = TensorDataset(test_input_ids, test_attention_masks, test_labels)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "# Test the model on the test set\n",
    "model.eval()\n",
    "test_preds = []\n",
    "test_true_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_dataloader:\n",
    "        input_ids, attention_masks, labels = batch\n",
    "        input_ids = input_ids.to(device)\n",
    "        attention_masks = attention_masks.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        outputs = model(input_ids, attention_masks)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "        test_preds.extend(predicted.cpu().numpy())\n",
    "        test_true_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "test_accuracy = accuracy_score(test_true_labels, test_preds)\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "# Convert predicted labels from integers to technique names\n",
    "predicted_labels_mapped = [id_to_label[label] for label in test_preds]\n",
    "true_labels_mapped = [id_to_label[label] for label in test_true_labels]\n",
    "\n",
    "# Print classification report for test set\n",
    "print(classification_report(true_labels_mapped, predicted_labels_mapped))\n",
    "\n",
    "# Create a DataFrame for actual vs. predicted labels\n",
    "test_results_df = pd.DataFrame({'Actual': true_labels_mapped, 'Predicted': predicted_labels_mapped})\n",
    "\n",
    "# Print the first 10 rows of the DataFrame\n",
    "print(test_results_df.head(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0d0b1cd",
   "metadata": {},
   "source": [
    "### Approach 2: CNN-BiLSTM for Propaganda Techniques Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4e189132",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/maicymaritim/anaconda3/lib/python3.11/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 81ms/step - accuracy: 0.0734 - loss: 2.2006 - val_accuracy: 0.0683 - val_loss: 2.2149\n",
      "Epoch 2/10\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 63ms/step - accuracy: 0.1190 - loss: 2.1610 - val_accuracy: 0.1180 - val_loss: 2.2041\n",
      "Epoch 3/10\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 66ms/step - accuracy: 0.2477 - loss: 2.0325 - val_accuracy: 0.3996 - val_loss: 1.9495\n",
      "Epoch 4/10\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 59ms/step - accuracy: 0.5199 - loss: 1.4871 - val_accuracy: 0.3168 - val_loss: 2.1111\n",
      "Epoch 5/10\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 63ms/step - accuracy: 0.7095 - loss: 0.8920 - val_accuracy: 0.3892 - val_loss: 1.9457\n",
      "Epoch 6/10\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 62ms/step - accuracy: 0.8541 - loss: 0.5104 - val_accuracy: 0.3851 - val_loss: 2.4308\n",
      "Epoch 7/10\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 62ms/step - accuracy: 0.9027 - loss: 0.3727 - val_accuracy: 0.3996 - val_loss: 2.6493\n",
      "Epoch 8/10\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 64ms/step - accuracy: 0.9473 - loss: 0.2354 - val_accuracy: 0.4203 - val_loss: 2.7394\n",
      "Epoch 9/10\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 65ms/step - accuracy: 0.9718 - loss: 0.1547 - val_accuracy: 0.4017 - val_loss: 3.1781\n",
      "Epoch 10/10\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 64ms/step - accuracy: 0.9821 - loss: 0.1025 - val_accuracy: 0.4493 - val_loss: 3.2299\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.4810 - loss: 3.1225\n",
      "Test Loss: 3.045464277267456\n",
      "Test Accuracy: 0.4810344874858856\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 44ms/step\n",
      "                           precision    recall  f1-score   support\n",
      "\n",
      " appeal_to_fear_prejudice       0.33      0.07      0.12        43\n",
      "causal_oversimplification       0.18      0.35      0.24        31\n",
      "                    doubt       0.26      0.24      0.25        38\n",
      "exaggeration,minimisation       0.13      0.21      0.16        28\n",
      "              flag_waving       0.55      0.46      0.50        39\n",
      "          loaded_language       0.09      0.11      0.10        37\n",
      "    name_calling,labeling       0.12      0.03      0.05        31\n",
      "           not_propaganda       0.75      0.71      0.73       301\n",
      "               repetition       0.24      0.41      0.30        32\n",
      "\n",
      "                 accuracy                           0.48       580\n",
      "                macro avg       0.29      0.29      0.27       580\n",
      "             weighted avg       0.51      0.48      0.48       580\n",
      "\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "                 Actual_Label            Predicted_Label\n",
      "0              not_propaganda             not_propaganda\n",
      "1              not_propaganda            loaded_language\n",
      "2                 flag_waving                flag_waving\n",
      "3    appeal_to_fear_prejudice  causal_oversimplification\n",
      "4             loaded_language             not_propaganda\n",
      "5              not_propaganda             not_propaganda\n",
      "6                  repetition                 repetition\n",
      "7   exaggeration,minimisation  causal_oversimplification\n",
      "8              not_propaganda                 repetition\n",
      "9             loaded_language             not_propaganda\n",
      "10             not_propaganda             not_propaganda\n",
      "11                flag_waving                flag_waving\n",
      "12      name_calling,labeling  exaggeration,minimisation\n",
      "13             not_propaganda             not_propaganda\n",
      "14  exaggeration,minimisation  causal_oversimplification\n",
      "15  exaggeration,minimisation             not_propaganda\n",
      "16                      doubt             not_propaganda\n",
      "17   appeal_to_fear_prejudice  causal_oversimplification\n",
      "18   appeal_to_fear_prejudice             not_propaganda\n",
      "19                      doubt             not_propaganda\n",
      "20                flag_waving                      doubt\n",
      "21             not_propaganda             not_propaganda\n",
      "22            loaded_language             not_propaganda\n",
      "23   appeal_to_fear_prejudice   appeal_to_fear_prejudice\n",
      "24                 repetition                 repetition\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Embedding, Conv1D, MaxPooling1D, LSTM, Dense, Dropout, Bidirectional\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "\n",
    "# Define propaganda techniques\n",
    "propaganda_techniques = ['not_propaganda', 'exaggeration,minimisation', 'causal_oversimplification',\n",
    "                         'name_calling,labeling', 'loaded_language', 'appeal_to_fear_prejudice',\n",
    "                         'flag_waving', 'repetition', 'doubt']\n",
    "\n",
    "# Tokenize the text data\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer()\n",
    "tokenizer.fit_on_texts(train_df[\"tagged_in_context\"])\n",
    "train_sequences = tokenizer.texts_to_sequences(train_df[\"tagged_in_context\"])\n",
    "test_sequences = tokenizer.texts_to_sequences(test_df[\"tagged_in_context\"])\n",
    "\n",
    "# Pad the sequences to a fixed length\n",
    "max_length = 100\n",
    "train_data = pad_sequences(train_sequences, maxlen=max_length)\n",
    "test_data = pad_sequences(test_sequences, maxlen=max_length)\n",
    "\n",
    "# Manually map the labels to integers\n",
    "label_to_id = {label: idx for idx, label in enumerate(propaganda_techniques)}\n",
    "train_labels = train_df[\"label\"].map(label_to_id)\n",
    "test_labels = test_df[\"label\"].map(label_to_id)\n",
    "\n",
    "# Convert the labels to numpy arrays\n",
    "train_labels = train_labels.to_numpy()\n",
    "test_labels = test_labels.to_numpy()\n",
    "\n",
    "# Split the data into train and validation sets (80:20 ratio)\n",
    "train_data, val_data, train_labels, val_labels = train_test_split(train_data, train_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Calculate class weights\n",
    "class_weights = {}\n",
    "for i, technique in enumerate(propaganda_techniques):\n",
    "    class_weights[i] = len(train_labels) / (len(propaganda_techniques) * (train_labels == i).sum())\n",
    "\n",
    "# Define the CNN-BiLSTM model\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=128, input_length=max_length))\n",
    "model.add(Conv1D(filters=64, kernel_size=3, activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Bidirectional(LSTM(64)))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(len(propaganda_techniques), activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model with class weights\n",
    "history = model.fit(train_data, train_labels, epochs=10, batch_size=32, validation_data=(val_data, val_labels), class_weight=class_weights)\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(test_data, test_labels, batch_size=32)\n",
    "print(\"Test Loss:\", loss)\n",
    "print(\"Test Accuracy:\", accuracy)\n",
    "\n",
    "# Predictions\n",
    "predictions = model.predict(test_data)\n",
    "predicted_labels = np.argmax(predictions, axis=1)\n",
    "\n",
    "# Convert predicted labels to technique names\n",
    "predicted_labels_mapped = [propaganda_techniques[label] for label in predicted_labels]\n",
    "\n",
    "# Convert true labels to technique names\n",
    "true_labels_mapped = [propaganda_techniques[label] for label in test_labels]\n",
    "\n",
    "# Classification report\n",
    "print(classification_report(true_labels_mapped, predicted_labels_mapped))\n",
    "\n",
    "# Create a DataFrame to store actual vs predicted labels during validation\n",
    "val_predictions = model.predict(val_data)\n",
    "val_predicted_labels = np.argmax(val_predictions, axis=1)\n",
    "val_predicted_labels_mapped = [propaganda_techniques[label] for label in val_predicted_labels]\n",
    "\n",
    "# Map numerical labels to their corresponding propaganda techniques\n",
    "actual_labels_mapped = [propaganda_techniques[label] for label in val_labels]\n",
    "\n",
    "#Create a DataFrame to store actual vs predicted labels during validation\n",
    "result_df = pd.DataFrame({'Actual_Label': actual_labels_mapped, 'Predicted_Label': val_predicted_labels_mapped})\n",
    "print(result_df.head(25))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "567b6937",
   "metadata": {},
   "source": [
    "#### Model Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "52701370",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "                           precision    recall  f1-score   support\n",
      "\n",
      " appeal_to_fear_prejudice       0.33      0.07      0.12        43\n",
      "causal_oversimplification       0.18      0.35      0.24        31\n",
      "                    doubt       0.26      0.24      0.25        38\n",
      "exaggeration,minimisation       0.13      0.21      0.16        28\n",
      "              flag_waving       0.55      0.46      0.50        39\n",
      "          loaded_language       0.09      0.11      0.10        37\n",
      "    name_calling,labeling       0.12      0.03      0.05        31\n",
      "           not_propaganda       0.75      0.71      0.73       301\n",
      "               repetition       0.24      0.41      0.30        32\n",
      "\n",
      "                 accuracy                           0.48       580\n",
      "                macro avg       0.29      0.29      0.27       580\n",
      "             weighted avg       0.51      0.48      0.48       580\n",
      "\n",
      "                Actual_Label        Predicted_Label\n",
      "0             not_propaganda         not_propaganda\n",
      "1  causal_oversimplification            flag_waving\n",
      "2   appeal_to_fear_prejudice         not_propaganda\n",
      "3             not_propaganda         not_propaganda\n",
      "4                 repetition  name_calling,labeling\n",
      "5      name_calling,labeling        loaded_language\n",
      "6            loaded_language         not_propaganda\n",
      "7             not_propaganda         not_propaganda\n",
      "8                flag_waving             repetition\n",
      "9                      doubt             repetition\n"
     ]
    }
   ],
   "source": [
    "# Predictions on test data\n",
    "test_predictions = model.predict(test_data)\n",
    "test_predicted_labels = np.argmax(test_predictions, axis=1)\n",
    "test_predicted_labels_mapped = [propaganda_techniques[label] for label in test_predicted_labels]\n",
    "\n",
    "# Convert true labels to technique names\n",
    "test_true_labels_mapped = [propaganda_techniques[label] for label in test_labels]\n",
    "\n",
    "# Classification report for test set\n",
    "print(classification_report(test_true_labels_mapped, test_predicted_labels_mapped))\n",
    "\n",
    "# Create a DataFrame to store actual vs predicted labels for test set\n",
    "test_result_df = pd.DataFrame({'Actual_Label': test_true_labels_mapped, 'Predicted_Label': test_predicted_labels_mapped})\n",
    "\n",
    "# Display first 10 rows of the DataFrame\n",
    "print(test_result_df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f27147f5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
